# src/eda.py
from pathlib import Path
import re, json
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns
from wordcloud import WordCloud
from tqdm import tqdm

from preprocess import clean_text

DATA_TSV = Path("data/train_snli.txt")
OUT_DIR  = Path("output/eda")
OUT_DIR.mkdir(parents=True, exist_ok=True)

REPORT = OUT_DIR / "eda_report.md"

def load_tsv(tsv_path: Path) -> pd.DataFrame:
    rows = []
    with tsv_path.open("r", encoding="utf-8") as f:
        for line in f:
            parts = line.rstrip("\n").split("\t")
            if len(parts) != 3:
                continue
            s, p, y = parts
            try:
                y = int(y)
            except:
                continue
            rows.append((s, p, y))
    return pd.DataFrame(rows, columns=["source_txt", "plagiarism_txt", "label"])

def plot_class_distribution(df: pd.DataFrame, outpath: Path):
    plt.figure(figsize=(5,4))
    sns.countplot(x="label", data=df)
    plt.title("Label Distribution (0=Not Plagiarized, 1=Plagiarized)")
    plt.xlabel("label")
    plt.ylabel("count")
    plt.tight_layout()
    plt.savefig(outpath, dpi=160)
    plt.close()

def plot_lengths(df: pd.DataFrame, outpath: Path):
    df = df.copy()
    df["source_len"] = df["source_txt"].str.split().map(len)
    df["plag_len"]   = df["plagiarism_txt"].str.split().map(len)

    plt.figure(figsize=(8,4))
    sns.kdeplot(df["source_len"], label="Source", fill=True)
    sns.kdeplot(df["plag_len"], label="Plagiarism", fill=True)
    plt.axvline(df["source_len"].mean(), ls="--", label=f"Avg source={df['source_len'].mean():.2f}")
    plt.axvline(df["plag_len"].mean(),   ls="--", label=f"Avg plag={df['plag_len'].mean():.2f}")
    plt.title("Text Length Distribution (words)")
    plt.legend()
    plt.tight_layout()
    plt.savefig(outpath, dpi=160)
    plt.close()

def make_wordcloud(text: str, outpath: Path):
    wc = WordCloud(width=1000, height=500, background_color="white").generate(text)
    wc.to_file(str(outpath))

def build_report(stats: dict, paths: dict, outpath: Path):
    md = f"""# EDA Report

**Rows:** {stats['rows_before']} → after cleaning: {stats['rows_after']}

## Missing / Duplicates
- Missing values (before drop):  
  source_txt={stats['na_source']}  |  plagiarism_txt={stats['na_plag']}  |  label={stats['na_label']}
- Duplicates dropped: {stats['dups']}

## Class Distribution
![class dist]({paths['class_distribution']})

## Text Lengths
- Avg source length: **{stats['avg_source_len']:.2f}** words  
- Avg plagiarism length: **{stats['avg_plag_len']:.2f}** words  
![lengths]({paths['text_lengths']})

## Wordclouds
**Source text**
![wc source]({paths['wc_source']})

**Plagiarized text**
![wc plag]({paths['wc_plag']})

*Generated by `src/eda.py`.*
"""
    outpath.write_text(md, encoding="utf-8")

def main():
    assert DATA_TSV.exists(), f"Put train_snli.txt in {DATA_TSV}"
    df = load_tsv(DATA_TSV)

    stats = {
        "rows_before": len(df),
        "na_source": df["source_txt"].isna().sum(),
        "na_plag": df["plagiarism_txt"].isna().sum(),
        "na_label": df["label"].isna().sum(),
    }

    # basic cleaning
    df.dropna(inplace=True)
    dups = df.duplicated().sum()
    df.drop_duplicates(inplace=True)
    stats["dups"] = int(dups)
    stats["rows_after"] = len(df)

    # plots
    class_png = OUT_DIR / "class_distribution.png"
    len_png   = OUT_DIR / "text_lengths.png"
    plot_class_distribution(df, class_png)
    plot_lengths(df, len_png)

    # wordclouds (light preprocessing for better quality)
    from itertools import islice
    src_clean = " ".join(map(clean_text, islice(df["source_txt"].tolist(), 0, len(df))))
    plag_clean = " ".join(map(clean_text, islice(df["plagiarism_txt"].tolist(), 0, len(df))))
    wc_src = OUT_DIR / "wordcloud_source.png"
    wc_plg = OUT_DIR / "wordcloud_plagiarized.png"
    make_wordcloud(src_clean, wc_src)
    make_wordcloud(plag_clean, wc_plg)

    # stats for lengths
    df_tmp = df.copy()
    df_tmp["source_len"] = df_tmp["source_txt"].str.split().map(len)
    df_tmp["plag_len"] = df_tmp["plagiarism_txt"].str.split().map(len)
    stats["avg_source_len"] = float(df_tmp["source_len"].mean())
    stats["avg_plag_len"]   = float(df_tmp["plag_len"].mean())

    build_report(
        stats,
        paths={
            "class_distribution": class_png.relative_to(OUT_DIR),
            "text_lengths": len_png.relative_to(OUT_DIR),
            "wc_source": wc_src.relative_to(OUT_DIR),
            "wc_plag": wc_plg.relative_to(OUT_DIR),
        },
        outpath=REPORT
    )
    print(f"EDA complete → {REPORT}")
    print("Artifacts:", *[str(p) for p in OUT_DIR.glob('*.png')], sep="\n - ")

if __name__ == "__main__":
    main()
